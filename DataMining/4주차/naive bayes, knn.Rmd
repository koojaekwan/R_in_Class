---
title: "naive bayes, knn"
author: "Jae Kwan Koo"
output:
  html_document:
    fig_height: 6
    fig_width: 10
    highlight: textmate
    toc: yes
    toc_depth: 4
    toc_float: yes
  word_document: default
  github_document:
    toc: yes
    toc_depth: 4
---  

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=T, fig.align = "center", message=F, warning=F, fig.height = 5, cache=F, dpi = 300, dev = "png")
```  

## 예측모형 개발속도 향상  

```{r}
library(doSNOW)

num_cores <- parallel:::detectCores()

cl <- makeCluster(num_cores-1, type = "SOCK")
registerDoSNOW(cl)
```  

본인처럼 참을성이 없는 사람들은 멀티코어를 활용해야 한다.  
본인은 1개의 코어는 다른 일을 하도록 두고 나머지 코어들을 R연산에 집중시켰다.  

동일한 예측모형을 교차검증 데이터를 만들어서 데이터만 달리해서 적합시키는 것이라 멀티코어를 갖춘 최근 컴퓨터 환경에 적합하다. 이를 위해서 멀티코어를 활용하여 병렬컴퓨팅으로 예측모형을 개발하여 예측모형 개발에 소요되는 시간을 대폭 줄인다.  

예측모형 caret 주요 함수를 실행하기에 앞서 `doSNOW`를 사용하는데 우선 코어숫자를 파악해야 되기 때문에 `parallel:::detectCores()` 함수로 예측모형 구축에 활용되는 하드웨어 CPU에서 코어숫자를 파악하고 클러스터를 구축해서 예측모형을 돌리면 된다.  


## Library  

```{r warning=FALSE, message=FALSE}
library(ISLR)  # Smarket data

# manupulate
library(tidyverse)
library(data.table)

# modeling
library(caret)
library(e1071)  # naive bayes
library(naivebayes)  # naive bayes  <- i will use this package
library(pROC)  # ROC curve
```


```{r}
setwd("D:\\Jae Kwan\\4학년1학기\\데이터마이닝입문 노윤환\\4주차")
```

## 당뇨병 데이터  

### Introduction  

iris데이터와 함께하는 실습은 질릴대로 질렸으므로 새로운 데이터를 한번 들고와봤다.  
당뇨병 데이터인데 예전에 혼자 logistic regression을 공부할 때, 사용해본 적이 있다.  
다운받아 한 번 실습을 해보고 더 좋은 성능이 나오면 말해줬으면 좋겠다.  

```{r}
data <- fread("dataset_37_diabetes.csv", data.table = F)

str(data)
data$class <- as.factor(data$class)


data %>% head
```  

[변수설명](https://github.com/koojaekwan/Rprogramming/blob/master/%EC%A7%84%ED%96%89%EC%A4%91/%EB%8B%B9%EB%87%A8%EB%B3%91-%EC%98%88%EC%B8%A1-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D---%EC%A7%84%ED%96%89%EC%A4%91.md#introduction)  

이와 관련된 EDA나 변수설명은 이 문서를 참고해보자.(추가 업데이트는 고려해볼만함)  


## Split the data(train, test)  

```{r}
set.seed(100)
index <- createDataPartition(data$class, p=0.7, list=F)

train <- data[index,]
test <- data[-index,]
```  

`caret`패키지로 데이터를 train, test분리할 수 있다.  
sample함수를 이용하여도 무방하다.  


## Naive Bayes using `naivebayes` package  

```{r}
model <- naive_bayes(class~., data = train)
# model1 <- naive_bayes(class~., data = train, laplace = 1) 

par(mfrow=c(2,2))
plot(model)
```  

test와 train의 변수별 분포를 볼 수 있다. 비슷한 분포이면 대충 잘 분리한 것처럼 보인다.  



```{r}
prediction <- predict(model, newdata = test[,1:ncol(test)-1], type = "class")

# predict(model, newdata = test[,1:ncol(test)-1] , type = "prob")  
# prob : compute the posterior prob


confusionMatrix(prediction,test$class)
```  

`type = class`를 두면 tested_negative인지 tested_positive인지 범주를 반환한다.  
`tepe = prob`을 두면 사후확률이 나오게 된다.  

정확도는 `r confusionMatrix(prediction,test$class)$overall[1]`  



## K-fold cross-validation  

```{r}
set.seed(100)
cv <- createFolds(data$class, k=10)

cv %>% str
```  

k-fold cross-validation 방법은 서로 다른 train데이터의 부분집합에서의 모형 성능을 평가한다.  
그리고 평균 예측 오차율을 계산한다.  
이 알고리즘은 다음과 같다.  

1. 무작위로 데이터셋을 k개의 부분집합으로 만든다.(예건대 5개의 부분집합(k=5))  
2. 한 부분집합을 보존하고 다른 나머지 부분집합들로 모형을 훈련시킨다.  
3. 보존된 부분집합의 데이터로 모형을 테스트하고 예측오차를 기록한다.  
4. 이 과정을 k개의 부분집합 각각이 test과정을 수행할 때까지 반복한다.  
5. k개의 기록된 오차들의 평균을 계산한다. This is called `the cross-validation error` serving as the performance metric for the model.  
6. K-fold cross-validation (CV) is a `robust method` for estimating the accuracy of a model.  

LOOCV와 비교했을 때, k-fold CV의 가장 명백한 장점은 계산적인 측면이다.  
덜 분명하지만 k-fold CV의 잠재적으로 더 중요한 장점은 이 것은 LOOCV보다 test error rate의 더 정확한 추정치를 제공한다는 것이다(James et al. 2014).  



### 어떻게 그러면 k를 선택할 것인가?  

k가 작은 값이면 더 편향되므로 적절하지 않다. 반면에 k가 높은 값이면 덜 편향되지만 높은 분산을 가지게 할 수 있다.  
k 값이 작을수록 (k = 2) 항상 validation 설정 접근 방식으로 하는 반면, k 값이 클수록 (k = 데이터 포인트 수) LOOCV 접근 방식으로 하게 된다.  

관습적으로, k=5 or k=10을 사용하는 k-fold CV를 일반적으로 수행한다.  
따라서 이 값들은 경험적으로 매우 높은 편향이나 매우 높은 편차를 가지지 않는 test error estimates를 산출하는 것으로 나타났다.  

다음으로 우리가 할 것은 예측오차를 추정하기 위한 10-fold CV이다.  
seed를 설정하여 재현성을 갖도 해보자.  



```{r}
k <- 10
result <- NA

for (i in 1:k) {
  
    test_idx <- cv[[i]]
    data_train <- data[-test_idx, ]
    data_test <- data[test_idx, ]
    
    # 모델링
    model <- naive_bayes(class~., data = data_train)
    prediction <- predict(model, newdata = data_test[,1:ncol(data_test)-1], type = "class")
    
    # 평가
    result[i] <- mean(prediction==data_test$class)
  }
```  

```{r}
result

result %>% mean # mean accuracy of 10 fold
result %>% sd # sd accuracy of 10 fold
```  

각 fold마다의 accuracy를 나타내고 있다.  

## caret을 이용한 naive bayes  


```{r}
modelLookup("nb")
```  

`modelLookup`함수를 이용하여 naive bayes모델에서 turnGrid에 어떤 인자를 넣을지 확인해보자  

`usekernel` parameter allows us to use a kernel density estimate for continuous variables versus a guassian density estimate  

`adjust` allows us to adjust the bandwidth of the kernel density (larger numbers mean more flexible density estimate)  

`fL` allows us to incorporate the Laplace smoother  





```{r warning=FALSE}
set.seed(100)

# Train the model
model <- train(class ~., data = train, method = "nb")

# Summarize the results
confusionMatrix(predict(model, test), test$class)
```  

어떤 값이 전혀 나타나지 않는다면 사후확률도 0이 될 것이다. 따라서 아주작은 값(1)을 더해줌으로 이를 예방할 수 있다.(fL인자)  
**naive_bayes(class~., data = train, laplace = 1)** 이전의 함수에서는 laplace인자로 설정한다.  

## Grid Search  

관심 있는 매개변수들을 대상으로 가능한 모든 조합을 시도하여 최적의 매개변수를 찾는 방법이다.  
매개변수를 튜닝하여 일반화 성능을 개선해준다.  

```{r warning=FALSE}
set.seed(100)

train.control <- trainControl(method = "cv", number = 10)
search_grid <- expand.grid(
  usekernel = c(TRUE, FALSE),
  fL = 0:5,
  adjust = seq(0, 5, by = 1)
)


model_cv1 <- train(x=train[, -ncol(train)], 
                  y=train$class, 
                  method = "nb",
                  trControl = train.control,
                  tuneGrid = search_grid)
                  #preProc = c("BoxCox", "center", "scale", "pca"))

trellis.par.set(caretTheme())
plot(model_cv1)  
```  

adjust, usekernel, fL의 값에 따라 Accuracy가 어떤지를 나타내고 있다.  

```{r}
ggplot(model_cv1)
```  

ggplot으로도 볼 수 있다.  
보니까 laplace는 어느값을 주더라도 상관이 없고, adjust가 1이고 kernel을 사용하지 않았을 때가 정확도가 가장 높다.  

```{r}
# results for best model

confusionMatrix(model_cv1)
```  

이 때의 정확도는 위와 같다.  


```{r}
# top 12 models

model_cv1$results %>% 
  top_n(10, wt = Accuracy) %>%
  arrange(desc(Accuracy))
```  

상위 정확도를 가지는 hyperparameter들의 조합이다.  


```{r}
confusionMatrix(predict(model_cv1, test), test$class)
```  

test에 적용한 결과이다.  





## KNN - Smarket data in ISLR  

```{r}
data <- Smarket
```  

```{r}
str(data)
```  

```{r}
boxplot(data[,-1])
boxplot(data[,"Year"])
```  

## Split the data(train, test)  

```{r}
index <- createDataPartition(Smarket$Direction, p = 0.75, list = F)
train_market <- Smarket[index, ]
test_market <- Smarket[-index, ]
```  

데이터의 75% 비율을 train에 할당. list형식으로 출력을 하지 않았다.  

```{r}
prop.table(table(train_market$Direction)) * 100
prop.table(table(test_market$Direction)) * 100
prop.table(table(Smarket$Direction)) * 100
```  

비슷한 비율로 **UP**과 **Down**이 train과 test셋에 분리가 됨  


```{r}
# 반응변수를 제외
trainX <- train_market[,names(train_market) != "Direction"]

preProcValues <- preProcess(x = trainX, method = c("center", "scale"))
preProcValues
```  

centering과 scaling을 동시에 하여 표준화를 시킴  

```{r}
modelLookup("knn")
```  

### Accuracy기준 k 설정  

```{r}
set.seed(100)

train.control <- trainControl(method="repeatedcv", 
                              number = 10, repeats = 3)

search_grid <-  expand.grid(k = 1:50) 


model_knn <- train(x = train_market[,-ncol(train_market)],
                   y = train_market$Direction,
                method = "knn", 
                trControl = train.control,
                preProcess = c("center","scale"), 
                tuneGrid = search_grid)
                # tuneLength = 20) # grid search하지 않을때 k갯수 간격 조정
```  

grid search를 통해 k에 따른 Accuracy를 확인해보았다.  
k=44일 때, best performance를 보여준다.  

```{r}
# k-NN 적합 결과
model_knn
plot(model_knn)
```  


```{r}
set.seed(100)
predict_knn <- predict(model_knn, newdata = test_market )

confusionMatrix(predict_knn, test_market$Direction )
# mean(predict_knn == test_market$Direction)
```  

### ROC기준 k설정  

```{r}
set.seed(100)

train.control2 <- trainControl(method="repeatedcv", 
                              number = 10, repeats = 3,
                              classProbs=TRUE,
                              summaryFunction = twoClassSummary)
search_grid2 <-  expand.grid(k = 1:50) 

# 추가 가능 옵션: classProbs=TRUE,summaryFunction = twoClassSummary
model_knn2 <- train(x = train_market[,-ncol(train_market)],
                   y = train_market$Direction,
                method = "knn", 
                trControl = train.control2,
                preProcess = c("center","scale"), 
                tuneGrid = search_grid2)
                # tuneLength = 20) # grid search하지 않을때 k갯수 간격 조정
```  

summaryFunction = twoClassSummary : AUC계산, 민감도, 특이도 등 체크  


```{r}
model_knn2
plot(model_knn2)
```  

```{r}
set.seed(100)
predict_knn2 <- predict(model_knn2, newdata = test_market)

confusionMatrix(predict_knn2, test_market$Direction )
```  

정분류율이 살짝 올라갔다.  
민감도는 클수록 좋다  


|지표|설명|값|
|:---:|:---:|:---|
|Accuracy 정확도|전체 예측에서 옳은 예측의 비율|(123+160)/(123+2+27+160)|
|Precision 정밀도|True라고 예측했던 값들 중 실제 True의 개수|123/(123+2)|
|Sensitivity (=recall) 민감도|실제로 True인 것 중 예측이 True인 비율|123/(123+27)|
|specificity 특이도|실제로 False인 것 중 예측이 False인 비율|160/(2+160)|
|FP Rate(False Alarm Rate|T가 아닌 T로 예측된 비율(1-specificity)|2/(2+160)|  


* F1점수  

precision과 recall의 조화평균  
시스템의 성능을 하나로 수치로 표현하기 위해 사용하는 점수로, 0~1의 값을 가진다.  
precision과 recall중 하나의 값이 클 때보다 두 값이 골고루 클 때 큰 값을 갖는다.  

$$
2\times {precision \times recall \over precision+recall}
$$  

* Kappa  

코헨의 카파 두 평가의 평가가 얼마나 일치하는 평가하는 값으로 0~1사이의 값을 가진다.  
$p(e)$는 두 평가자의 평가가 우연히 일치할 확률을 뜻하며, 코헨의 카파는 두 평가자의 평가가 우연히 일치할 확률을 제외한 뒤의 점수다.  

$$
\kappa = {accuracy -p(e) \over 1-p(e)]}
$$  









```{r}
test_market %>% group_by(Direction) %>% summarise(n=n())
```  

위 결과에서 No Information Rate에 대해서 추가로 살펴보자. No Information Rate는 가장 많은 값이 발견된 분류의 비율이다.  
이 예의 test셋에서 Down이 150개, Up이 162개 있었다. 이런 데이터가 주어졌을 때 가장 간단한 분류 알고리즘은 입력이 무엇이든 항상 1을 출력하는 것이다.  
데이터에서 분류 1의 비율이 분류 0의 비율보다 높으므로 정확도가 50%는 넘을 것이기 때문이다.   항상 1을 결과로 출력하는 분류 알고리즘의 정확도는 162/(150+162)=`r 162/(150+162)`이며 No Information Rate는 바로 이 값에 해당한다.  

실제 분류 알고리즘은 피처를 들여다보고 예측을 수행하므로 분류의 비율만 보고 결과를 출력하는 단순한 분류 알고리즘보다 성능이 좋아야 한다. 따라서 `r 162/(150+162)`은 모델을 만들었을 때 무조건 넘어야 하는 정확도다.  

## ROC  

```{r}
ROC <- 
  roc(test_market$Direction, 
      predict(model_knn2, newdata = test_market, type = "prob")[,"Down"],
      levels = levels(test_market$Direction)) %>% 
  plot(col = "blue", 
       print.thres = 0.5)  # 기준값 0.5일때의 결과를 표시

AUC <- 
roc(test_market$Direction, 
    predict(model_knn2, newdata = test_market, type = "prob")[,"Down"],
    levels = levels(test_market$Direction)) %>% 
  auc


ROC
AUC
```  

## Comparision  

```{r}
# # 4. 모형 비교평가-----
# model_list <- list(
#   cv = model_knn2,
#   cv2 = model_knn2
# )
# 
# resamps <- resamples(model_list)
# 
# summary(resamps)
# dotplot(resamps, metric = "ROC")
```  

다른 머신러닝 기법들과 비교할 때 쓰면 되겠다.  
여기서는 하나만 하여 비교할 대상이 없다.  



## Refer  

[k-fold CV](http://www.sthda.com/english/articles/38-regression-model-validation/157-cross-validation-essentials-in-r/)  

[Naïve Bayes Classifier](https://uc-r.github.io/naive_bayes)  

[caret book](https://topepo.github.io/caret/index.html)  

[caret 사용기 github](https://statkclee.github.io/model/model-caret-build.html)  

[confusionMatrix](https://medium.com/@hslee09/r-%EB%B6%84%EB%A5%98-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98-%EB%AA%A8%EB%8D%B8-%ED%8F%89%EA%B0%80-%EB%B0%A9%EB%B2%95-1a8f3c7913a3)  





