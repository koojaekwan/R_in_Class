---
title: "Artificial neural network"
author: "Jae Kwan Koo"
output:
  html_document:
    fig_height: 6
    fig_width: 10
    highlight: textmate
    toc: yes
    toc_depth: 4
    toc_float: yes
  word_document: default
  github_document:
    toc: yes
    toc_depth: 4
---  

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=T, fig.align = "center", message=F, warning=F, fig.height = 5, cache=F, dpi = 300, dev = "png")
```  

## Library  

```{r message=FALSE, warning=FALSE}
library(devtools)

library(tidyverse)
library(data.table)
library(MASS) # Boston data

library(caret)
library(nnet)  # 순전파
library(neuralnet) # 역전파
```  

## Data - iris  

```{r}
data(iris)
```  



## `nnet()`  

순전파를 이용한 모델링  

```{r warning=FALSE, message=FALSE, fig.width=5, fig.height=5}
set.seed(100)
nn.iris <- nnet(Species~., data=iris, size=3, rang=.1, decay=5e-4, maxit=200)
summary(nn.iris)


#source_url('https://gist.githubusercontent.com/fawda123/7471137/raw/466c1474d0a505ff044412703516c34f1a4684a5/nnet_plot_update.r')
source_url('https://gist.githubusercontent.com/Peque/41a9e20d6687f2f3108d/raw/85e14f3a292e126f1454864427e3a189c2fe33f3/nnet_plot_update.r')

plot.nnet(nn.iris)
```  

size는 layer을 의미한다. size = c(3,2)로 두면 2개의 층에 각 층마다 3개의 노드 2개의 노드가 있다는 것  
rang : 초기치  
decay : tolerance  
maxit : 최대 반복횟수  

```{r}
confusionMatrix(predict(nn.iris, iris, type = "class") %>% factor, iris$Species)
```


```{r}
set.seed(100)
# function evaluating test error rates for different numbers of hidden units
test.err <- function(h.size){
  ir <- nnet(Species~., data=iris, size = h.size,
            decay = 5e-4, trace=F)
  y <- iris$Species
  p <- predict(ir, iris, type = "class")
  err <- mean(y != p)
  c(h.size, err)
}

# compare test error rates for neural networks with 2-10 hidden units

# t(sapply(2:10, FUN = test.err))
# map(.x = 2:10, .f = test.err)

out <- 
data.frame(matrix(unlist(map(.x = 2:10, .f = test.err)), 
                  nrow=length(map(.x = 2:10, .f = test.err)), byrow=T),
           stringsAsFactors=FALSE)
# data.frame(t(sapply(map(.x = 2:10, .f = test.err),c)))


plot(out, type="b", xlab="The number of Hidden units", ylab="Test Error")
```  


```{r}
set.seed(100)
nn.iris <- nnet(Species~., data=iris, size=c(4), rang=.1, decay=5e-4, maxit=200)

confusionMatrix(predict(nn.iris, iris, type = "class") %>% factor, iris$Species)
```  


## caret  

### `caret package` 예측모형 개발속도 향상  

```{r}
library(doSNOW)

num_cores <- parallel:::detectCores()

cl <- makeCluster(num_cores-1, type = "SOCK")
registerDoSNOW(cl)
```  

### nnet에 쓰이는 hyperparameter 확인  

```{r}
modelLookup("nnet")
```  

## modeling  

```{r}
set.seed(100)

# train.control <- trainControl(method = "cv", number = 10)

search_grid <- expand.grid(
  size = 2:10,
  decay = 5e-4)


model_nnet <- train(x = iris[, -ncol(iris)], 
                  y = iris$Species, 
                  method = "nnet", 
                  maxit = 300,
                  linout = F,
                  trControl = trainControl(verboseIter=TRUE), # summary시 결과보여줌
                  tuneGrid = search_grid,
                  trace=F)

summary(model_nnet)
```  

```{r}
trellis.par.set(caretTheme())
plot(model_nnet)  
```  


```{r}
confusionMatrix(predict(model_nnet, iris), iris$Species)
```  

## `neuralnet()`  

역전파를 이용한 모델링  

```{r}
net.iris <- neuralnet(Species~., hidden = c(2,2), data = iris, 
                      linear.output = F)

predict_net <- apply(predict(net.iris, iris[1:4]),1,which.max)
# compute(net.iris, iris)$net.result

a<-factor(predict_net, labels = c("setosa", "versicolor", "virginica"))
b<-iris$Species 

confusionMatrix(a,b)
```  

분류모형에서는 linear.output = F로 둔다.  
hidden은 nnet에서의 size와 같은 역할이다.  
stepmax또한 nnet에서 maxit과 같다.  


```{r}
# koo_grid <- function(your_hidden){
#   model_neural <- neuralnet(Species~., hidden = your_hidden, data = iris,
#                             linear.output = F, stepmax = 1000, 
#                             rep=2, threshold = 1)
#   
#   predict_net <- apply(predict(net.iris, iris), 1, which.max)
#   a<-factor(predict_net, labels = c("setosa", "versicolor", "virginica"))
#   b<-iris$Species 
#   
#   confusion_table <- confusionMatrix(a,b)
#   
#   accuracy <- sum(diag(confusion_table$table))/sum(confusion_table$table)
#   error <- 1-accuracy
#   
#   return(c(your_hidden, error))
# }
# 
# sapply(1:10, koo_grid)
```




## Data - infert  

```{r}
data(infert)

net.infert <- neuralnet(case~age+parity+induced+spontaneous, 
                        hidden=c(2,3), data=infert, linear.output=F)



plot(net.infert)

head(net.infert$generalized.weights[[1]])
```  


```{r}
results <- data.frame(actual = infert$case, 
                      prediction = predict(net.infert, infert))

results$prediction <- ifelse(results$prediction>=0.5,1,0)

results$actual <- as.factor(results$actual)
results$prediction <- as.factor(results$prediction)


confusionMatrix(results[,1], results[,2])
```  


```{r}
par(mfrow=c(2,2))

gwplot(net.infert, selected.covariate='age', min=-2.5, max=5)
gwplot(net.infert, selected.covariate='parity', min=-2.5, max=5)
gwplot(net.infert, selected.covariate='induced', min=-2.5, max=5)
gwplot(net.infert, selected.covariate='spontaneous', min=-2.5, max=5)
```  


**찾아보니 caret패키지의 train을 통한 neuralnet은 분류는 지원하지 않는다.**  


## neuralnet with Boston data  

자세한 설명은 예전에 썻던 [전산통계 - ANN](https://github.com/koojaekwan/R_in_Class/blob/master/Computerized_Statistics/chapter8/Artificial-Neural-Network-practice--2-.md) 을 참조하자.  

```{r}
head(Boston)

apply(Boston,2,function(x) sum(is.na(x)))
```  

```{r}
index <- createDataPartition(Boston$medv, p = 0.75, list = F)
train_boston <- Boston[index,]
test_boston <- Boston[-index,]
lm.fit <- glm(medv~., data = train_boston)

pr.lm <- predict(lm.fit, test_boston)
MSE.lm <- sum((pr.lm - test_boston$medv)^2)/nrow(test_boston)

summary(lm.fit)
```  

`glm()`은 generalized linear model을 가리키며, 반응변수가 범주형 이산형 등 일반화선형모형을 적합한다.  인자 `family` 반응변수의 분포를 이야기한다. 기본적으로 gaussian을 채택하고 있다. 즉, 기본값인 경우에 `lm()`함수와 같은 역할을 하며 선형모형을 적합한다.  

null deviance : 모든 회귀계수가 0일때 데비언스(under H0)  
residal deviance : 현재 모형에서의 데비언스  

귀무가설 하에서보다 현재 모형에서의 데비언스가 더 작다. 모형이 적합하다는 것을 암시한다.  
상세하게는 카이제곱값과 비교해야 한다.  





```{r}
maxs <- apply(Boston, 2, max) 
mins <- apply(Boston, 2, min)
scaled <- as.data.frame(scale(Boston, center = mins, scale = maxs - mins))

train_scaled <- scaled[index,]
test_scaled <- scaled[-index,]


f <- as.formula(paste("medv ~", paste(names(train_scaled)[!names(train_scaled) %in% "medv"], collapse = " + ")))

net.Boston <- neuralnet(f, data=train_scaled, hidden=c(5,3), linear.output=T)

plot(net.Boston)
net.Boston$result.matrix
```  

여기서는 표준화 말고 정규화를 했다.  

표준화(standardization) : 각 observation이 평균을 기준으로 어느 정도 떨어져 있는지를 나타낼때 사용된다. 값의 스케일이 다른 두 개의 변수가 있을 때, 이 변수들의 스케일 차이를 제거해 주는 효과가 있다. 제로 평균 으로부터 각 값들의 분산을 나타낸다. 각 요소의 값에서 평균을 뺀 다음 표준편차로 나누어 준다.  

표준정규분포로 만듦. 0을중심 좌우대칭 => 핸들링용이, 분석용이, 자료 특성을 파악하기에도 좋다. 스케일이 없어져서 좋기도 하다.  

<br>

정규화(normalization) : 정규화는 데이터의 범위를 0과 1로 변환하여 데이터 분포를 조정하는 방법이다.  
여러개의 feature에서 값들의 범주를 일치시킬 수 있어 분석하기 더 용이해질 것이다.   
최솟값을 뺐기 때문에 0보다는 큰 수가 나오게 된다.  


```{r}
pr.nn <- compute(net.Boston, test_scaled[,1:13])

pr.nn_ <- pr.nn$net.result*(max(Boston$medv)-min(Boston$medv))+min(Boston$medv)
test.r <- (test_scaled$medv)*(max(Boston$medv)-min(Boston$medv))+min(Boston$medv)

MSE.nn <- sum((test.r - pr.nn_)^2)/nrow(test_scaled)



c(MSE.lm,MSE.nn)
```  

MSE가 더 작은 모형이 이 예제에서 더 좋은 모형이라고 말할 수 있을 것이다.  

```{r}
par(mfrow=c(1,2))

plot(test_boston$medv, pr.nn_, col='red', main='Real vs predicted NN', pch=18,cex=0.7)
abline(0,1,lwd=2)
legend('bottomright',legend='NN',pch=18,col='red', bty='n')

plot(test_boston$medv, pr.lm, col='blue', main='Real vs predicted lm', pch=18, cex=0.7)
abline(0,1,lwd=2)
legend('bottomright',legend='LM',pch=18,col='blue', bty='n', cex=.95)
```  

```{r}
plot(test_boston$medv, pr.nn_, col='red', main='Real vs predicted NN', pch=18, cex=1)

points(test_boston$medv, pr.lm, col='blue', pch=18, cex=1)
abline(0,1,lwd=2)
legend('bottomright',legend=c('NN','LM'), pch=18, col=c('red','blue'))
```  


