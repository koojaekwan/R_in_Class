---
title: "chap 1. Multivariate data analysis"
author: "Jae Kwan Koo"
output:
  github_document:
    toc: yes
    toc_depth: 4
  word_document: default
  html_document:
    fig_height: 6
    fig_width: 10
    highlight: textmate
    toc: yes
    toc_depth: 4
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=T, fig.align = "center", message=F, warning=F, fig.height = 5, cache=F, dpi = 300, dev = "png")
```  

[R과 함께하는 다변량 자료분석] 의 연습문제 1.7에 해당하는 분석이다.  



```{r}
library(data.table)  
library(corrplot)    # making the correlation plot.
library(tidyverse)

library(MVN)         # Multivariate normality test
```  

```{r echo=FALSE}
setwd("D:\\Jae Kwan\\github\\R_in_class\\Multivariate Analysis\\data")
getwd()
```  


```{r}
data<-fread("klpga.txt", drop="V1") # delete the number column.
data[1:6]  
```  

```{r}
str(data)
dim(data)
colSums(is.na(data))
```  

```{r}
summary(data)
```  

```{r}
data %>% 
  select_if(is.numeric) %>%
  cor(use="complete.obs") %>%
  corrplot.mixed(tl.cex=0.85)
```




### Find mean vector, covariance matrix S, correlation matrix R  

```{r}
X<-as.matrix(data)
n<-nrow(X)

xbar<-t(X)%*%matrix(1,n,1)/n

I<-diag(n)
J<-matrix(1,n,n)
H<-I-1/n*J

Y<-H%*%X
S<-t(Y)%*%Y/(n-1)

D<-diag(1/sqrt(diag(S)))
Z<-H%*%X%*%D
R<-t(Z)%*%Z/(n-1)


colnames(xbar)<-c("Mean")



xbar; S; R
```  

- Another way to solve the problem.  

```{r eval=FALSE}
colMeans(data)
cov(data)
cor(data)
```  

### Find two kinds of multivariate variations, and interpret them.  


```{r}
det(S)   #generalized variance
det(R) 

sum(diag(S))  #total variance
sum(diag(R))
```  

Two methods were obtained: covariance matrix and correlation matrix.  
generalized variances can be obtained as a matrix of covariance or as a matrix of correlation.  
since the determinant of R(correlation matrix) is near zero, the correlation between the variables is very high.  
it is obvious that the total variance of the correlation matrix is 6. because we can see the number of variables is 6.  

#### refer  

| type | description |  
|--| :- | 
| generalized variance | determinant of covariance matrix | 
| total variance | sum of diag element of covariance matrix |  

that is, generalized variance use all the information of the variance and covariance.  
while, |S| = 0 indicates that the columns in the centralization data matrix Y are linear dependents, indicating that there is co-linearity among the variables.  

In particular, since |S| and tr(S) are heavily influenced by the size of the variance, it is desirable to use a correlation R with variance 1 as diagonal elements.  


### Between centering & standardizing , which one is more appropriate for the data?  explain it.  

It seems more appropriate to standardize each variable because of its different units.  
in particular, the variance of the prize rate is larger than others.  


###Is it possible to group variables in the data into two parts of skill factor group and award factor group?  


```{r}
plot(data)

round(cor(data),3)
```  

When checking the correlation coefficient with the prize rate within the variables of a group of technology factor variables, only the average number of putts is negative, so it cannot be divided into a group of technical factors and a group of performance factors based on the association.  

### Check the multivariate normality based on , and kurtosis & skewness.  

```{r}
result<-mvn(data, multivariatePlot =  "qq")
result
```  


Since p-value about skewness and kurtosis is lower than alpha=0.05, we reject the null hypothesis H0.  
That is , Multivariate data are not normally distributed.

| hypothesis |  |  
|--| :- | 
| H0 | "Multivariate data are normally distributed." | 
| H1 | "Multivariate data are not normally distributed." |  

```{r}
n<-dim(data)[[1]]
p<-dim(data)[[2]]


s<-cov(data)
xbar<-colMeans(data)

m<-mahalanobis(data,xbar,s)
m<-sort(m)
id<-seq(1,n)
pt<-(id-0.5)/n
q<-qchisq(pt,p)

plot(q,m,pch='*')
abline(0,1)
```  

The horizontal axis in this plot indicates quantile of chi square distribution and verticle axis represents  Mahalanobis distance.  



```{r}
rq<-cor(cbind(q,m))[1,2]
rq
```  

Almost of coordinate points are not out of linearity.  
Since correlation coefficient of quantile and mahalanobis distance is almost 1, the linearity of the chi-square plot is highly appreciated.  


